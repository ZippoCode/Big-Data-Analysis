{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "TRAIN AND TEST\n",
    "\"\"\"\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "dataset = pd.read_csv('datasets/wine.data', header=None)\n",
    "array = dataset.values\n",
    "X = array[:, 1:]\n",
    "Y = array[:, 0]\n",
    "model = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.932203389831\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "sklearn.model_selection.train_test_split(*arrays, **options)\n",
    "    \n",
    "    Split arrays or matrices into random train and test subsets\n",
    "    \n",
    "    Quick utility that wraps input validation and next(ShuffleSplit().split(X, y))and application to \n",
    "    input data into a single call for splitting (and optionally subsampling) data in a oneliner.\n",
    "    \n",
    "    Read more in the User Guide.\n",
    "Parameters:\n",
    "    - *arrays : sequence of indexables with same length / shape[0]\n",
    "        Allowed inputs are lists, numpy arrays, scipy-sparse matrices or pandas dataframes.\n",
    "    - test_size : float, int, None, optional\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include \n",
    "        in the test split. If int, represents the absolute number of test samples. If None, the value is set \n",
    "        to the complement of the train size. By default, the value is set to 0.25. The default will change \n",
    "        in version 0.21.It will remain 0.25 only if train_size is unspecified, otherwise it will \n",
    "        complement the specified train_size.\n",
    "     - train_size : float, int, or None, default None\n",
    "        If float, should be between 0.0 and 1.0 and represent the proportion of the dataset to include \n",
    "        in the train split. If int, represents the absolute number of train samples. If None, the value \n",
    "        is automatically set to the complement of the test size.\n",
    "    - random_state : int, RandomState instance or None, optional (default=None)\n",
    "        If int, random_state is the seed used by the random number generator; If RandomState instance, \n",
    "        random_state is the random number generator; If None, the random number generator is the RandomState \n",
    "        instance used by np.random.\n",
    "    - shuffle : boolean, optional (default=True)\n",
    "        Whether or not to shuffle the data before splitting. If shuffle=False then stratify must be None.\n",
    "    - stratify : array-like or None (default is None)\n",
    "        If not None, data is split in a stratified fashion, using this as the class labels.\n",
    "Returns:\n",
    "    - splitting : list, length=2 * len(arrays)\n",
    "        List containing train-test split of inputs.\n",
    "        \n",
    "        New in version 0.16: If the input is sparse, the output will be a scipy.sparse.csr_matrix. \n",
    "        Else, output type is the same as the input type.\n",
    "\"\"\"\n",
    "from sklearn.model_selection import train_test_split\n",
    "test_size = 0.33\n",
    "seed = 7\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size = test_size, random_state = seed)\n",
    "model.fit(X_train, y_train)\n",
    "result = model.score(X_test, y_test)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.643 (5.249)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "sklearn.model_selection.cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=None, n_jobs=1, \n",
    "                                        verbose=0, fit_params=None, pre_dispatch=‘2*n_jobs’)\n",
    "\n",
    "    Evaluate a score by cross-validation\n",
    "\n",
    "    Read more in the User Guide.\n",
    "\n",
    "Parameters:\n",
    "    - estimator : estimator object implementing ‘fit’\n",
    "        The object to use to fit the data.\n",
    "    - X : array-like\n",
    "        The data to fit. Can be for example a list, or an array.\n",
    "    - y : array-like, optional, default: None\n",
    "            The target variable to try to predict in the case of supervised learning.\n",
    "    - groups : array-like, with shape (n_samples,), optional\n",
    "            Group labels for the samples used while splitting the dataset into train/test set.\n",
    "    - scoring : string, callable or None, optional, default: None\n",
    "        A string (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y).\n",
    "    - cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
    "        \n",
    "            * None, to use the default 3-fold cross validation, \n",
    "            * integer, to specify the number of folds in a (Stratified)KFold,\n",
    "            * An object to be used as a cross-validation generator.\n",
    "            * An iterable yielding train, test splits.\n",
    "    \n",
    "        For integer/None inputs, if the estimator is a classifier and y is either binary or multiclass, \n",
    "        StratifiedKFold is used. In all other cases, KFold is used.\n",
    "    \n",
    "        Refer User Guide for the various cross-validation strategies that can be used here.\n",
    "    - n_jobs : integer, optional\n",
    "        The number of CPUs to use to do the computation. -1 means ‘all CPUs’.\n",
    "    - verbose : integer, optional\n",
    "        The verbosity level.\n",
    "    - fit_params : dict, optional\n",
    "        Parameters to pass to the fit method of the estimator.\n",
    "    - pre_dispatch : int, or string, optional\n",
    "        Controls the number of jobs that get dispatched during parallel execution. Reducing this \n",
    "        number can be useful to avoid an explosion of memory consumption when more jobs get dispatched \n",
    "        than CPUs can process. This parameter can be:\n",
    "    \n",
    "        None, in which case all the jobs are immediately created and spawned. Use this for lightweight \n",
    "        and fast-running jobs, to avoid delays due to on-demand spawning of the jobs\n",
    "        \n",
    "        An int, giving the exact number of total jobs that are spawned\n",
    "        \n",
    "        A string, giving an expression as a function of n_jobs, as in ‘2*n_jobs’\n",
    "\n",
    "Returns:\n",
    "    - scores : array of float, shape=(len(list(cv)),)\n",
    "        Array of scores of the estimator for each run of the cross validation.\n",
    "\"\"\"\n",
    "from sklearn.model_selection import cross_val_score\n",
    "num_folds = 10\n",
    "result = cross_val_score(model, X, Y, cv = num_folds)\n",
    "print('Accuracy: {:.03f} ({:.03f})'.format(result.mean()*100.0, result.std()*100.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 95.506 (20.718)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "class sklearn.model_selection.LeaveOneOut\n",
    "\n",
    "    Leave-One-Out cross-validator\n",
    "    \n",
    "    Provides train/test indices to split data in train/test sets. Each sample is used once as a test set (singleton) while the remaining samples form the training set.\n",
    "\n",
    "    Note: LeaveOneOut() is equivalent to KFold(n_splits=n) and LeavePOut(p=1) where n is the number of samples.\n",
    "\n",
    "    Due to the high number of test sets (which is the same as the number of samples) this cross-validation \n",
    "    method can be very costly. For large datasets one should favor KFold, ShuffleSplit or StratifiedKFold.\n",
    "\n",
    "    Read more in the User Guide.\n",
    "\"\"\"\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "loocv = LeaveOneOut()\n",
    "result = cross_val_score(model, X, Y, cv = loocv)\n",
    "print('Accuracy: {:.03f} ({:.03f})'.format(result.mean()*100.0, result.std()*100.0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
