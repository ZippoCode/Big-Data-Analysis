{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read DataFrame\n",
    "dataset = pd.read_csv('datasets/wine.data', sep=',', header=None)\n",
    "X = dataset.iloc[:, 1:]\n",
    "Y = dataset.iloc[:, :1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SELECT K BEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>127.0</td>\n",
       "      <td>3.06</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1065.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>2.76</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1050.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>101.0</td>\n",
       "      <td>3.24</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1185.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>113.0</td>\n",
       "      <td>3.49</td>\n",
       "      <td>7.80</td>\n",
       "      <td>1480.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>118.0</td>\n",
       "      <td>2.69</td>\n",
       "      <td>4.32</td>\n",
       "      <td>735.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0     1     2       3\n",
       "0  127.0  3.06  5.64  1065.0\n",
       "1  100.0  2.76  4.38  1050.0\n",
       "2  101.0  3.24  5.68  1185.0\n",
       "3  113.0  3.49  7.80  1480.0\n",
       "4  118.0  2.69  4.32   735.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "class sklearn.feature_selection.SelectKBest(score_func=<function f_classif>, k=10)\n",
    "\n",
    "    Select features according to the k highest scores.\n",
    "\n",
    "    Read more in the User Guide.\n",
    "\n",
    "Parameters:\n",
    "    - score_func : callable\n",
    "        Function taking two arrays X and y, and returning a pair of arrays (scores, pvalues) \n",
    "        or a single array with scores. Default is f_classif (see below “See also”). The default \n",
    "        function only works with classification tasks.\n",
    "    - k : int or “all”, optional, default=10\n",
    "        Number of top features to select. The “all” option bypasses selection, for use in a parameter search.\n",
    "        \n",
    "Methods\n",
    "    fit(X, y) : Run score function on (X, y) and get the appropriate features.\n",
    "    fit_transform(X[, y]) : Fit to data, then transform it.\n",
    "    get_params([deep]) : Get parameters for this estimator.\n",
    "    get_support([indices]) : Get a mask, or integer index, of the features selected\n",
    "    inverse_transform(X) : Reverse the transformation operation\n",
    "    set_params(**params) : Set the parameters of this estimator.\n",
    "    transform(X) : Reduce X to the selected features.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "test = SelectKBest(score_func=chi2, k=4)\n",
    "fit = test.fit(X, Y)\n",
    "features = test.transform(X)\n",
    "pd.DataFrame(features).head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RECURSIVE FEATURE ELIMINATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num Features: 3\n",
      "Selected Features: [False False  True False False False  True False False  True False False\n",
      " False]\n",
      "Features Ranking: [4 3 1 3 5 4 1 5 4 1 3 2 5]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zippo/anaconda3/lib/python3.6/site-packages/sklearn/utils/validation.py:578: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "class sklearn.feature_selection.RFE(estimator, n_features_to_select=None, step=1, verbose=0)\n",
    "\n",
    "\n",
    "    Feature ranking with recursive feature elimination.\n",
    "    Given an external estimator that assigns weights to features (e.g., the coefficients of a linear model), \n",
    "    the goal of recursive feature elimination (RFE) is to select features by recursively considering smaller \n",
    "    and smaller sets of features. First, the estimator is trained on the initial set of features and the \n",
    "    importance of each feature is obtained either through a coef_ attribute or through a \n",
    "    feature_importances_ attribute. Then, the least important features are pruned from current set of features. \n",
    "    That procedure is recursively repeated on the pruned set until the desired number of features to select \n",
    "    is eventually reached.\n",
    "\n",
    "    Read more in the User Guide.\n",
    "    \n",
    "Parameters:\n",
    "    - estimator : object\n",
    "        A supervised learning estimator with a fit method that provides information about feature \n",
    "        importance either through a coef_ attribute or through a feature_importances_ attribute.\n",
    "    - n_features_to_select : int or None (default=None)\n",
    "        The number of features to select. If None, half of the features are selected.\n",
    "    - step : int or float, optional (default=1)\n",
    "        If greater than or equal to 1, then step corresponds to the (integer) number of features \n",
    "        to remove at each iteration. If within (0.0, 1.0), then step corresponds to the percentage \n",
    "        (rounded down) of features to remove at each iteration.\n",
    "    - verbose : int, default=0\n",
    "        Controls verbosity of output.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "rfe = RFE(model, n_features_to_select=3, step=3)\n",
    "rfe.fit(X, Y)\n",
    "print(\"Num Features: {}\".format(rfe.n_features_))\n",
    "print(\"Selected Features: {}\".format(rfe.support_))\n",
    "print(\"Features Ranking: {}\".format(rfe.ranking_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Principal component analysis (PCA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "class sklearn.decomposition.PCA(n_components=None, copy=True, whiten=False, svd_solver=’auto’, tol=0.0, iterated_power=’auto’, random_state=None)[source]\n",
    "\n",
    "\n",
    "    Linear dimensionality reduction using Singular Value Decomposition of the data to project it to a \n",
    "    lower dimensional space.\n",
    "\n",
    "    It uses the LAPACK implementation of the full SVD or a randomized truncated SVD by the method of \n",
    "    Halko et al. 2009, depending on the shape of the input data and the number of components to extract.\n",
    "\n",
    "    It can also use the scipy.sparse.linalg ARPACK implementation of the truncated SVD.\n",
    "\n",
    "    Notice that this class does not support sparse input. See TruncatedSVD for an alternative with sparse data.\n",
    "\n",
    "    Read more in the User Guide.\n",
    "\n",
    "Parameters:\n",
    "    - n_components : int, float, None or string\n",
    "        Number of components to keep. if n_components is not set all components are kept:\n",
    "            n_components == min(n_samples, n_features)\n",
    "        if n_components == ‘mle’ and svd_solver == ‘full’, Minka’s MLE is used to guess the dimension \n",
    "        if 0 < n_components < 1 and svd_solver == ‘full’, select the number of components such that the \n",
    "        amount of variance that needs to be explained is greater than the percentage specified by \n",
    "        n_components n_components cannot be equal to n_features for svd_solver == ‘arpack’.\n",
    "    - copy : bool (default True)\n",
    "        If False, data passed to fit are overwritten and running fit(X).transform(X) will not yield the\n",
    "        expected results, use fit_transform(X) instead.\n",
    "    - whiten : bool, optional (default False)\n",
    "        When True (False by default) the components_ vectors are multiplied by the square root of \n",
    "        n_samples and then divided by the singular values to ensure uncorrelated outputs with unit \n",
    "        component-wise variances.\n",
    "\n",
    "        Whitening will remove some information from the transformed signal (the relative variance scales \n",
    "        of the components) but can sometime improve the predictive accuracy of the downstream estimators\n",
    "        by making their data respect some hard-wired assumptions.\n",
    "    - svd_solver : string {‘auto’, ‘full’, ‘arpack’, ‘randomized’}\n",
    "        * auto :\n",
    "            the solver is selected by a default policy based on X.shape and n_components: \n",
    "            if the input data is larger than 500x500 and the number of components to extract is lower\n",
    "            than 80% of the smallest dimension of the data, then the more efficient ‘randomized’ method\n",
    "            is enabled. Otherwise the exact full SVD is computed and optionally truncated afterwards.\n",
    "        * full :\n",
    "            run exact full SVD calling the standard LAPACK solver via scipy.linalg.svd and select \n",
    "            the components by postprocessing\n",
    "    - arpack :\n",
    "        run SVD truncated to n_components calling ARPACK solver via scipy.sparse.linalg.svds. \n",
    "        It requires strictly 0 < n_components < X.shape[1]\n",
    "    - randomized :\n",
    "        run randomized SVD by the method of Halko et al.\n",
    "    - tol : float >= 0, optional (default .0)\n",
    "        Tolerance for singular values computed by svd_solver == ‘arpack’.\n",
    "    - iterated_power : int >= 0, or ‘auto’, (default ‘auto’)\n",
    "        Number of iterations for the power method computed by svd_solver == ‘randomized’.\n",
    "    - random_state : int, RandomState instance or None, optional (default None)\n",
    "        If int, random_state is the seed used by the random number generator; If RandomState instance, \n",
    "        random_state is the random number generator; If None, the random number generator is the RandomState \n",
    "        instance used by np.random. Used when svd_solver == ‘arpack’ or ‘randomized’.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=3)\n",
    "fit = pca.fit(X)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
