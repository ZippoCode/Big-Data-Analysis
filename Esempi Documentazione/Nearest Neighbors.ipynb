{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gli algoritmi del package sklean.neighbors forniscono funzionalità per i metodi di learning basati sul\n",
    "neigbors sia per quanto riguarda quelli supervisionati che non. Il Nearest Neighbors non supervisionato\n",
    "sono alla base della maggiorparte dei metodi di apprendimento, maggiormente per quanto riguarda i manifold \n",
    "e lo spectal clustering. Mentre il Nearest Neighbors supervisionato viene utilizziato per la classificazione\n",
    "di dati con labels discrete e per la regressione quanto i dati sono costituiti da labels continue.\n",
    "\n",
    "Il principio alla base del nearest neighbors è la ricerca di un predefinito numero di esempi del training \n",
    "pià vicini in termini di distanza dal nuovo punto, e predirre la label da questi. Il numero di esempi può\n",
    "essere definito dagli utenti (k-nearest neighbors) oppure varia basandosi sulla densità di punti locali \n",
    "(radius-based neighbors learning). La distanza, in generale, un misura metrica: ad esempio la distanza Euclidea\n",
    "e quella che viene maggiormente scelta. I metodi che si basano su Neighbors sono conosciuti come \n",
    "'non-generalizzati' perché semplicemente 'ricordano' tutti i dati del training set (eventualmente lo \n",
    "trasformano in una struttura indicizzata come Ball Tree o KD Tree).\n",
    "\n",
    "Nonostante la sua semplicità, il Nearest Neighbors ha avuto successo in un gran numero di problemi di \n",
    "classifiazione e regressione, ad esempio per il riconoscimento di numeri scritti a mano oppure sull'analisi\n",
    "di immagini satellitali. L'essere un metodo non parametrico gli fa avere successo nelle situazioni di \n",
    "classificazione dove il confine di decisione di una classe è molto irregolare.\n",
    "\n",
    "La classe in sklearn.neighbors può gestire sia gli array di Numpy che le metrici di scipy.sparse. Per le \n",
    "matrici dense, supporta un gran numero di misure di distanze. Per le matrici sparse, le matrici di \n",
    "Minkowsku sono supportate per la ricerca.\n",
    "\n",
    "Ci sono molti routine learning che si basano sul Nearest Neighbors. Ad esempio lo stimatore di densità\n",
    "kernel.\n",
    "\"\"\"\n",
    "print ('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupevised Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "NearestNeighbors implementa il metodo di apprendimento del nearest neighbors non supervisionato. Esso \n",
    "\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
