{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Esempi presi da: \n",
    "http://scikit-learn.org/stable/modules/preprocessing.html\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "4.3 PREPROCESSING DATA\n",
    "\n",
    "    The sklearn.preprocessing package provides several common utility functions and transformer classes \n",
    "    to change raw feature vectors into a representation that is more suitable for the downstream estimators.\n",
    "\n",
    "    In general, learning algorithms benefit from standardization of the data set. If some outliers are present in \n",
    "    the set, robust scalers or transformers are more appropriate. The behaviors of the different scalers, \n",
    "    transformers, and normalizers on a dataset containing marginal outliers is highlighted in Compare the effect\n",
    "    of different scalers on data with outliers.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "4.3.1 STANDARDIZATION, OR MEAN REMOVAL AND VARIANCE SCALING\n",
    "\n",
    "    Standardization of datasets is a common requirement for many machine learning estimators \n",
    "    implemented in scikit-learn; they might behave badly if the individual features do not more or less look \n",
    "    like standard normally distributed data: Gaussian with zero mean and unit variance.\n",
    "\n",
    "    In practice we often ignore the shape of the distribution and just transform the data to center\n",
    "    it by removing the mean value of each feature, then scale it by dividing non-constant features \n",
    "    by their standard deviation.\n",
    "\n",
    "    For instance, many elements used in the objective function of a learning algorithm (such as the RBF\n",
    "    kernel of Support Vector Machines or the l1 and l2 regularizers of linear models) assume that all \n",
    "    features are centered around zero and have variance in the same order. If a feature has a variance \n",
    "    that is orders of magnitude larger than others, it might dominate the objective function and make \n",
    "    the estimator unable to learn from other features correctly as expected.\n",
    "\n",
    "    The function scale provides a quick and easy way to perform this operation on a single array-like dataset:\n",
    "\n",
    "\"\"\"\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "\n",
    "X_train = np.array([[ 1., -1.,  2.],\n",
    "                    [ 2.,  0.,  0.],\n",
    "                    [ 0.,  1., -1.]])\n",
    "\n",
    "X_scaled = preprocessing.scale(X_train)\n",
    "X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  0.])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scaled data has zero mean and unit variance:\n",
    "X_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.,  1.,  1.])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The preprocessing module further provides a utility class StandardScaler that implements \n",
    "    the Transformer API to compute the mean and standard deviation on a training set so as to be able \n",
    "    to later reapply the same transformation on the testing set. This class is hence suitable for use \n",
    "    in the early steps of a sklearn.pipeline.Pipeline:\n",
    "\"\"\"\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.        ,  0.        ,  0.33333333])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.mean_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.81649658,  0.81649658,  1.24721913])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        , -1.22474487,  1.33630621],\n",
       "       [ 1.22474487,  0.        , -0.26726124],\n",
       "       [-1.22474487,  1.22474487, -1.06904497]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.44948974,  1.22474487, -0.26726124]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The scaler instance can then be used on new data to transform it the same way it did on the \n",
    "    training set:\n",
    "\"\"\"\n",
    "X_test = [[-1.,1,0.]]\n",
    "scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    It is possible to disable either centering or scaling by either passing with_mean=False or\n",
    "    with_std=False to the constructor of StandardScaler.\n",
    "\"\"\"\n",
    "scaler = preprocessing.StandardScaler(X_train, with_mean= False, with_std= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5       ,  0.        ,  1.        ],\n",
       "       [ 1.        ,  0.5       ,  0.33333333],\n",
       "       [ 0.        ,  1.        ,  0.        ]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.3.1.1 SCALING FEATURES TO A RANGE\n",
    "\n",
    "    An alternative standardization is scaling features to lie between a given minimum and maximum value, \n",
    "    often between zero and one, or so that the maximum absolute value of each feature is scaled to unit size. \n",
    "    This can be achieved using MinMaxScaler or MaxAbsScaler, respectively.\n",
    "\n",
    "    The motivation to use this scaling include robustness to very small standard deviations of features\n",
    "    and preserving zero entries in sparse data.\n",
    "\n",
    "    Here is an example to scale a toy data matrix to the [0, 1] range:\n",
    "\n",
    "\"\"\"\n",
    "min_max_scaler = preprocessing.MinMaxScaler()\n",
    "X_train_minmax = min_max_scaler.fit_transform(X_train)\n",
    "X_train_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5       ,  0.        ,  1.66666667]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The same instance of the transformer can then be applied to some new test data unseen during the \n",
    "    fit call: the same scaling and shifting operations will be applied to be consistent with \n",
    "    the transformation performed on the train data:\n",
    "\"\"\"\n",
    "X_test = np.array([[-3., -1., 4.]])\n",
    "X_test_minmax = min_max_scaler.transform(X_test)\n",
    "X_test_minmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5       ,  0.5       ,  0.33333333])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    It is possible to introspect the scaler attributes to find about the exact nature of \n",
    "    the transformation learned on the training data:\n",
    "\"\"\"\n",
    "min_max_scaler.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.        ,  0.5       ,  0.33333333])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_max_scaler.min_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "    If MinMaxScaler is given an explicit feature_range=(min, max) the full formula is:\n",
    "\"\"\"\n",
    "X = X_train\n",
    "X_std = (X - X.min(axis=0)) / (X.max(axis=0) - X.min(axis=0))\n",
    "min = 1\n",
    "max = 2\n",
    "X_scaled = X_std * (max - min) + min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.5, -1. ,  1. ],\n",
       "       [ 1. ,  0. ,  0. ],\n",
       "       [ 0. ,  1. , -0.5]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    MaxAbsScaler works in a very similar fashion, but scales in a way that the training data lies within \n",
    "    the range [-1, 1] by dividing through the largest maximum value in each feature. It is meant for data \n",
    "    that is already centered at zero or sparse data.\n",
    "\n",
    "    Here is how to use the toy data from the previous example with this scaler:\n",
    "\"\"\"\n",
    "max_abs_scaled = preprocessing.MaxAbsScaler()\n",
    "X_train_maxabs = max_abs_scaled.fit_transform(X_train)\n",
    "X_train_maxabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.5, -1. ,  2. ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_maxabs = max_abs_scaled.transform(X_test)\n",
    "X_test_maxabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.,  1.,  2.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_abs_scaled.scale_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.3,  5.1,  5.8,  6.5,  7.9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    As with scale, the module further provides convenience functions minmax_scale and maxabs_scale if \n",
    "    ou don’t want to create an object.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "4.3.1.2 SCALING SPARSE DATA\n",
    "\n",
    "    Centering sparse data would destroy the sparseness structure in the data, and thus rarely is a\n",
    "    sensible thing to do. However, it can make sense to scale sparse inputs, especially if features \n",
    "    are on different scales.\n",
    "\n",
    "    MaxAbsScaler and maxabs_scale were specifically designed for scaling sparse data, and are the \n",
    "    recommended way to go about this. However, scale and StandardScaler can accept scipy.sparse\n",
    "    matrices as input, as long as with_mean=False is explicitly passed to the constructor. Otherwise a\n",
    "    ValueError will be raised as silently centering would break the sparsity and would often crash the \n",
    "    execution by allocating excessive amounts of memory unintentionally. RobustScaler cannot be fitted to \n",
    "    sparse inputs, but you can use the transform method on sparse inputs.\n",
    "    \n",
    "    Note that the scalers accept both Compressed Sparse Rows and Compressed Sparse Columns \n",
    "    format (see scipy.sparse.csr_matrix and scipy.sparse.csc_matrix). Any other sparse input will be \n",
    "    converted to the Compressed Sparse Rows representation. To avoid unnecessary memory copies, it\n",
    "    is recommended to choose the CSR or CSC representation upstream.\n",
    "    \n",
    "    Finally, if the centered data is expected to be small enough, explicitly converting the input \n",
    "    to an array using the toarray method of sparse matrices is another option.\n",
    "    \n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "4.3.1.3 SCALING DATA WITH OUTLIERS\n",
    "    \n",
    "    If your data contains many outliers, scaling using the mean and variance of the data is likely \n",
    "    to not work very well. In these cases, you can use robust_scale and RobustScaler as drop-in \n",
    "    replacements instead. They use more robust estimates for the center and range of your data.\n",
    "    \n",
    "    REFERENCE: Further discussion on the importance of centering and scaling data is available on this FAQ:\n",
    "    Should I normalize/standardize/rescale the data?\n",
    "    \n",
    "    Scaling vs Whitening\n",
    "    \n",
    "    It is sometimes not enough to center and scale the features independently, since a downstream \n",
    "    model can further make some assumption on the linear independence of the features.\n",
    "\n",
    "    To address this issue you can use sklearn.decomposition.PCA or sklearn.decomposition.RandomizedPCA \n",
    "    with whiten=True to further remove the linear correlation across features.\n",
    "    \n",
    "    Scaling target variables in regression\n",
    "    \n",
    "    scale and StandardScaler work out-of-the-box with 1d arrays. This is very useful for scaling \n",
    "    the target / response variables used for regression.\n",
    "    \n",
    "4.3.2 NON-LINEAR TRANSFORMATION\n",
    "\n",
    "    Like scalers, QuantileTransformer puts each feature into the same range or distribution. \n",
    "    However, by performing a rank transformation, it smooths out unusual distributions and is less \n",
    "    influenced by outliers than scaling methods. It does, however, distort correlations and \n",
    "    distances within and across features.\n",
    "    \n",
    "    QuantileTransformer and quantile_transform provide a non-parametric transformation based on \n",
    "    the quantile function to map the data to a uniform distribution with values between 0 and 1:\n",
    "\"\"\"\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import  train_test_split\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state = 0)\n",
    "quantile_transformer = preprocessing.QuantileTransformer(random_state = 0)\n",
    "X_train_trans = quantile_transformer.fit_transform(X_train)\n",
    "X_test_trans = quantile_transformer.transform(X_test)\n",
    "np.percentile(X_train[:,0], [0, 25, 50, 75, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  9.99999998e-08,   2.38738739e-01,   5.09009009e-01,\n",
       "         7.43243243e-01,   9.99999900e-01])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    This feature corresponds to the sepal length in cm. Once the quantile transformation applied, \n",
    "    those landmarks approach closely the percentiles previously defined:\n",
    "\"\"\"\n",
    "np.percentile(X_train_trans[:,0], [0, 25, 50, 75, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.4  ,  5.125,  5.75 ,  6.175,  7.3  ])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    This can be confirmed on a independent testing set with similar remarks:\n",
    "\"\"\"\n",
    "np.percentile(X_test[:,0], [0, 25, 50, 75, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.01351351,  0.25012513,  0.47972973,  0.6021021 ,  0.94144144])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.percentile(X_test_trans[:, 0], [0, 25, 50, 75, 100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    Thus the median of the input becomes the mean of the output, centered at 0. The normal output \\n    is clipped so that the input’s minimum and maximum — corresponding to the 1e-7 and 1 - 1e-7 \\n    quantiles respectively — do not become infinite under the transformation.\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    It is also possible to map the transformed data to a normal distribution by setting \n",
    "    output_distribution='normal':\n",
    "\"\"\"\n",
    "quantile_transformer = preprocessing.QuantileTransformer(\n",
    "    output_distribution = 'normal', random_state = 0)\n",
    "X_trans = quantile_transformer.fit_transform(X)\n",
    "quantile_transformer.quantiles_\n",
    "\"\"\"\n",
    "    Thus the median of the input becomes the mean of the output, centered at 0. The normal output \n",
    "    is clipped so that the input’s minimum and maximum — corresponding to the 1e-7 and 1 - 1e-7 \n",
    "    quantiles respectively — do not become infinite under the transformation.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80377277,  0.55160877,  0.22064351,  0.0315205 ],\n",
       "       [ 0.82813287,  0.50702013,  0.23660939,  0.03380134],\n",
       "       [ 0.80533308,  0.54831188,  0.2227517 ,  0.03426949],\n",
       "       [ 0.80003025,  0.53915082,  0.26087943,  0.03478392],\n",
       "       [ 0.790965  ,  0.5694948 ,  0.2214702 ,  0.0316386 ],\n",
       "       [ 0.78417499,  0.5663486 ,  0.2468699 ,  0.05808704],\n",
       "       [ 0.78010936,  0.57660257,  0.23742459,  0.0508767 ],\n",
       "       [ 0.80218492,  0.54548574,  0.24065548,  0.0320874 ],\n",
       "       [ 0.80642366,  0.5315065 ,  0.25658935,  0.03665562],\n",
       "       [ 0.81803119,  0.51752994,  0.25041771,  0.01669451],\n",
       "       [ 0.80373519,  0.55070744,  0.22325977,  0.02976797],\n",
       "       [ 0.786991  ,  0.55745196,  0.26233033,  0.03279129],\n",
       "       [ 0.82307218,  0.51442011,  0.24006272,  0.01714734],\n",
       "       [ 0.8025126 ,  0.55989251,  0.20529392,  0.01866308],\n",
       "       [ 0.81120865,  0.55945424,  0.16783627,  0.02797271],\n",
       "       [ 0.77381111,  0.59732787,  0.2036345 ,  0.05430253],\n",
       "       [ 0.79428944,  0.57365349,  0.19121783,  0.05883625],\n",
       "       [ 0.80327412,  0.55126656,  0.22050662,  0.04725142],\n",
       "       [ 0.8068282 ,  0.53788547,  0.24063297,  0.04246464],\n",
       "       [ 0.77964883,  0.58091482,  0.22930848,  0.0458617 ],\n",
       "       [ 0.8173379 ,  0.51462016,  0.25731008,  0.03027177],\n",
       "       [ 0.78591858,  0.57017622,  0.23115252,  0.06164067],\n",
       "       [ 0.77577075,  0.60712493,  0.16864581,  0.03372916],\n",
       "       [ 0.80597792,  0.52151512,  0.26865931,  0.07901744],\n",
       "       [ 0.776114  ,  0.54974742,  0.30721179,  0.03233808],\n",
       "       [ 0.82647451,  0.4958847 ,  0.26447184,  0.03305898],\n",
       "       [ 0.79778206,  0.5424918 ,  0.25529026,  0.06382256],\n",
       "       [ 0.80641965,  0.54278246,  0.23262105,  0.03101614],\n",
       "       [ 0.81609427,  0.5336001 ,  0.21971769,  0.03138824],\n",
       "       [ 0.79524064,  0.54144043,  0.27072022,  0.03384003],\n",
       "       [ 0.80846584,  0.52213419,  0.26948861,  0.03368608],\n",
       "       [ 0.82225028,  0.51771314,  0.22840286,  0.06090743],\n",
       "       [ 0.76578311,  0.60379053,  0.22089897,  0.0147266 ],\n",
       "       [ 0.77867447,  0.59462414,  0.19820805,  0.02831544],\n",
       "       [ 0.81803119,  0.51752994,  0.25041771,  0.01669451],\n",
       "       [ 0.82512295,  0.52807869,  0.19802951,  0.03300492],\n",
       "       [ 0.82699754,  0.52627116,  0.19547215,  0.03007264],\n",
       "       [ 0.81803119,  0.51752994,  0.25041771,  0.01669451],\n",
       "       [ 0.80212413,  0.54690282,  0.23699122,  0.03646019],\n",
       "       [ 0.80779568,  0.53853046,  0.23758697,  0.03167826],\n",
       "       [ 0.80033301,  0.56023311,  0.20808658,  0.04801998],\n",
       "       [ 0.86093857,  0.44003527,  0.24871559,  0.0573959 ],\n",
       "       [ 0.78609038,  0.57170209,  0.23225397,  0.03573138],\n",
       "       [ 0.78889479,  0.55222635,  0.25244633,  0.09466737],\n",
       "       [ 0.76693897,  0.57144472,  0.28572236,  0.06015208],\n",
       "       [ 0.82210585,  0.51381615,  0.23978087,  0.05138162],\n",
       "       [ 0.77729093,  0.57915795,  0.24385598,  0.030482  ],\n",
       "       [ 0.79594782,  0.55370283,  0.24224499,  0.03460643],\n",
       "       [ 0.79837025,  0.55735281,  0.22595384,  0.03012718],\n",
       "       [ 0.81228363,  0.5361072 ,  0.22743942,  0.03249135],\n",
       "       [ 0.76701103,  0.35063361,  0.51499312,  0.15340221],\n",
       "       [ 0.74549757,  0.37274878,  0.52417798,  0.17472599],\n",
       "       [ 0.75519285,  0.33928954,  0.53629637,  0.16417236],\n",
       "       [ 0.75384916,  0.31524601,  0.54825394,  0.17818253],\n",
       "       [ 0.7581754 ,  0.32659863,  0.5365549 ,  0.17496355],\n",
       "       [ 0.72232962,  0.35482858,  0.57026022,  0.16474184],\n",
       "       [ 0.72634846,  0.38046824,  0.54187901,  0.18446945],\n",
       "       [ 0.75916547,  0.37183615,  0.51127471,  0.15493173],\n",
       "       [ 0.76301853,  0.33526572,  0.53180079,  0.15029153],\n",
       "       [ 0.72460233,  0.37623583,  0.54345175,  0.19508524],\n",
       "       [ 0.76923077,  0.30769231,  0.53846154,  0.15384615],\n",
       "       [ 0.73923462,  0.37588201,  0.52623481,  0.187941  ],\n",
       "       [ 0.78892752,  0.28927343,  0.52595168,  0.13148792],\n",
       "       [ 0.73081412,  0.34743622,  0.56308629,  0.16772783],\n",
       "       [ 0.75911707,  0.3931142 ,  0.48800383,  0.17622361],\n",
       "       [ 0.76945444,  0.35601624,  0.50531337,  0.16078153],\n",
       "       [ 0.70631892,  0.37838513,  0.5675777 ,  0.18919257],\n",
       "       [ 0.75676497,  0.35228714,  0.53495455,  0.13047672],\n",
       "       [ 0.76444238,  0.27125375,  0.55483721,  0.18494574],\n",
       "       [ 0.76185188,  0.34011245,  0.53057542,  0.14964948],\n",
       "       [ 0.6985796 ,  0.37889063,  0.56833595,  0.21312598],\n",
       "       [ 0.77011854,  0.35349703,  0.50499576,  0.16412362],\n",
       "       [ 0.74143307,  0.29421947,  0.57667016,  0.17653168],\n",
       "       [ 0.73659895,  0.33811099,  0.56754345,  0.14490471],\n",
       "       [ 0.76741698,  0.34773582,  0.51560829,  0.15588157],\n",
       "       [ 0.76785726,  0.34902603,  0.51190484,  0.16287881],\n",
       "       [ 0.76467269,  0.31486523,  0.53976896,  0.15743261],\n",
       "       [ 0.74088576,  0.33173989,  0.55289982,  0.18798594],\n",
       "       [ 0.73350949,  0.35452959,  0.55013212,  0.18337737],\n",
       "       [ 0.78667474,  0.35883409,  0.48304589,  0.13801311],\n",
       "       [ 0.76521855,  0.33391355,  0.52869645,  0.15304371],\n",
       "       [ 0.77242925,  0.33706004,  0.51963422,  0.14044168],\n",
       "       [ 0.76434981,  0.35581802,  0.51395936,  0.15814134],\n",
       "       [ 0.70779525,  0.31850786,  0.60162596,  0.1887454 ],\n",
       "       [ 0.69333409,  0.38518561,  0.57777841,  0.1925928 ],\n",
       "       [ 0.71524936,  0.40530797,  0.53643702,  0.19073316],\n",
       "       [ 0.75457341,  0.34913098,  0.52932761,  0.16893434],\n",
       "       [ 0.77530021,  0.28304611,  0.54147951,  0.15998258],\n",
       "       [ 0.72992443,  0.39103094,  0.53440896,  0.16944674],\n",
       "       [ 0.74714194,  0.33960997,  0.54337595,  0.17659719],\n",
       "       [ 0.72337118,  0.34195729,  0.57869695,  0.15782644],\n",
       "       [ 0.73260391,  0.36029701,  0.55245541,  0.1681386 ],\n",
       "       [ 0.76262994,  0.34186859,  0.52595168,  0.1577855 ],\n",
       "       [ 0.76986879,  0.35413965,  0.5081134 ,  0.15397376],\n",
       "       [ 0.73544284,  0.35458851,  0.55158213,  0.1707278 ],\n",
       "       [ 0.73239618,  0.38547167,  0.53966034,  0.15418867],\n",
       "       [ 0.73446047,  0.37367287,  0.5411814 ,  0.16750853],\n",
       "       [ 0.75728103,  0.3542121 ,  0.52521104,  0.15878473],\n",
       "       [ 0.78258054,  0.38361791,  0.4603415 ,  0.16879188],\n",
       "       [ 0.7431482 ,  0.36505526,  0.5345452 ,  0.16948994],\n",
       "       [ 0.65387747,  0.34250725,  0.62274045,  0.25947519],\n",
       "       [ 0.69052512,  0.32145135,  0.60718588,  0.22620651],\n",
       "       [ 0.71491405,  0.30207636,  0.59408351,  0.21145345],\n",
       "       [ 0.69276796,  0.31889319,  0.61579374,  0.1979337 ],\n",
       "       [ 0.68619022,  0.31670318,  0.61229281,  0.232249  ],\n",
       "       [ 0.70953708,  0.28008043,  0.61617694,  0.1960563 ],\n",
       "       [ 0.67054118,  0.34211284,  0.61580312,  0.23263673],\n",
       "       [ 0.71366557,  0.28351098,  0.61590317,  0.17597233],\n",
       "       [ 0.71414125,  0.26647062,  0.61821183,  0.19185884],\n",
       "       [ 0.69198788,  0.34599394,  0.58626751,  0.24027357],\n",
       "       [ 0.71562645,  0.3523084 ,  0.56149152,  0.22019275],\n",
       "       [ 0.71576546,  0.30196356,  0.59274328,  0.21249287],\n",
       "       [ 0.71718148,  0.31640359,  0.58007326,  0.22148252],\n",
       "       [ 0.6925518 ,  0.30375079,  0.60750157,  0.24300063],\n",
       "       [ 0.67767924,  0.32715549,  0.59589036,  0.28041899],\n",
       "       [ 0.69589887,  0.34794944,  0.57629125,  0.25008866],\n",
       "       [ 0.70610474,  0.3258945 ,  0.59747324,  0.1955367 ],\n",
       "       [ 0.69299099,  0.34199555,  0.60299216,  0.19799743],\n",
       "       [ 0.70600618,  0.2383917 ,  0.63265489,  0.21088496],\n",
       "       [ 0.72712585,  0.26661281,  0.60593821,  0.18178146],\n",
       "       [ 0.70558934,  0.32722984,  0.58287815,  0.23519645],\n",
       "       [ 0.68307923,  0.34153961,  0.59769433,  0.24395687],\n",
       "       [ 0.71486543,  0.25995106,  0.62202576,  0.18567933],\n",
       "       [ 0.73122464,  0.31338199,  0.56873028,  0.20892133],\n",
       "       [ 0.69595601,  0.3427843 ,  0.59208198,  0.21813547],\n",
       "       [ 0.71529453,  0.31790868,  0.59607878,  0.17882363],\n",
       "       [ 0.72785195,  0.32870733,  0.56349829,  0.21131186],\n",
       "       [ 0.71171214,  0.35002236,  0.57170319,  0.21001342],\n",
       "       [ 0.69594002,  0.30447376,  0.60894751,  0.22835532],\n",
       "       [ 0.73089855,  0.30454106,  0.58877939,  0.1624219 ],\n",
       "       [ 0.72766159,  0.27533141,  0.59982915,  0.18683203],\n",
       "       [ 0.71578999,  0.34430405,  0.5798805 ,  0.18121266],\n",
       "       [ 0.69417747,  0.30370264,  0.60740528,  0.2386235 ],\n",
       "       [ 0.72366005,  0.32162669,  0.58582004,  0.17230001],\n",
       "       [ 0.69385414,  0.29574111,  0.63698085,  0.15924521],\n",
       "       [ 0.73154399,  0.28501714,  0.57953485,  0.21851314],\n",
       "       [ 0.67017484,  0.36168166,  0.59571097,  0.2553047 ],\n",
       "       [ 0.69804799,  0.338117  ,  0.59988499,  0.196326  ],\n",
       "       [ 0.71066905,  0.35533453,  0.56853524,  0.21320072],\n",
       "       [ 0.72415258,  0.32534391,  0.56672811,  0.22039426],\n",
       "       [ 0.69997037,  0.32386689,  0.58504986,  0.25073566],\n",
       "       [ 0.73337886,  0.32948905,  0.54206264,  0.24445962],\n",
       "       [ 0.69052512,  0.32145135,  0.60718588,  0.22620651],\n",
       "       [ 0.69193502,  0.32561648,  0.60035539,  0.23403685],\n",
       "       [ 0.68914871,  0.33943145,  0.58629069,  0.25714504],\n",
       "       [ 0.72155725,  0.32308533,  0.56001458,  0.24769876],\n",
       "       [ 0.72965359,  0.28954508,  0.57909015,  0.22005426],\n",
       "       [ 0.71653899,  0.3307103 ,  0.57323119,  0.22047353],\n",
       "       [ 0.67467072,  0.36998072,  0.58761643,  0.25028107],\n",
       "       [ 0.69025916,  0.35097923,  0.5966647 ,  0.21058754]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "4.3.3 NORMALIZATION\n",
    "\n",
    "    Normalization is the process of scaling individual samples to have unit norm. \n",
    "    This process can be useful if you plan to use a quadratic form such as the dot-product \n",
    "    or any other kernel to quantify the similarity of any pair of samples.\n",
    "\n",
    "    This assumption is the base of the Vector Space Model often used in text classification \n",
    "    and clustering contexts.\n",
    "\n",
    "    The function normalize provides a quick and easy way to perform this operation on a single array-like \n",
    "    dataset, either using the l1 or l2 norms:\n",
    "\n",
    "\"\"\"\n",
    "X_normalized = preprocessing.normalize(X, norm='l2')\n",
    "X_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Normalizer(copy=True, norm='l2')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The preprocessing module further provides a utility class Normalizer that implements \n",
    "    the same operation using the Transformer API (even though the fit method is useless in this case: \n",
    "    the class is stateless as this operation treats samples independently).\n",
    "\n",
    "    This class is hence suitable for use in the early steps of a sklearn.pipeline.Pipeline:\n",
    "\"\"\"\n",
    "normalizer = preprocessing.Normalizer().fit(X)\n",
    "normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.80377277,  0.55160877,  0.22064351,  0.0315205 ],\n",
       "       [ 0.82813287,  0.50702013,  0.23660939,  0.03380134],\n",
       "       [ 0.80533308,  0.54831188,  0.2227517 ,  0.03426949],\n",
       "       [ 0.80003025,  0.53915082,  0.26087943,  0.03478392],\n",
       "       [ 0.790965  ,  0.5694948 ,  0.2214702 ,  0.0316386 ],\n",
       "       [ 0.78417499,  0.5663486 ,  0.2468699 ,  0.05808704],\n",
       "       [ 0.78010936,  0.57660257,  0.23742459,  0.0508767 ],\n",
       "       [ 0.80218492,  0.54548574,  0.24065548,  0.0320874 ],\n",
       "       [ 0.80642366,  0.5315065 ,  0.25658935,  0.03665562],\n",
       "       [ 0.81803119,  0.51752994,  0.25041771,  0.01669451],\n",
       "       [ 0.80373519,  0.55070744,  0.22325977,  0.02976797],\n",
       "       [ 0.786991  ,  0.55745196,  0.26233033,  0.03279129],\n",
       "       [ 0.82307218,  0.51442011,  0.24006272,  0.01714734],\n",
       "       [ 0.8025126 ,  0.55989251,  0.20529392,  0.01866308],\n",
       "       [ 0.81120865,  0.55945424,  0.16783627,  0.02797271],\n",
       "       [ 0.77381111,  0.59732787,  0.2036345 ,  0.05430253],\n",
       "       [ 0.79428944,  0.57365349,  0.19121783,  0.05883625],\n",
       "       [ 0.80327412,  0.55126656,  0.22050662,  0.04725142],\n",
       "       [ 0.8068282 ,  0.53788547,  0.24063297,  0.04246464],\n",
       "       [ 0.77964883,  0.58091482,  0.22930848,  0.0458617 ],\n",
       "       [ 0.8173379 ,  0.51462016,  0.25731008,  0.03027177],\n",
       "       [ 0.78591858,  0.57017622,  0.23115252,  0.06164067],\n",
       "       [ 0.77577075,  0.60712493,  0.16864581,  0.03372916],\n",
       "       [ 0.80597792,  0.52151512,  0.26865931,  0.07901744],\n",
       "       [ 0.776114  ,  0.54974742,  0.30721179,  0.03233808],\n",
       "       [ 0.82647451,  0.4958847 ,  0.26447184,  0.03305898],\n",
       "       [ 0.79778206,  0.5424918 ,  0.25529026,  0.06382256],\n",
       "       [ 0.80641965,  0.54278246,  0.23262105,  0.03101614],\n",
       "       [ 0.81609427,  0.5336001 ,  0.21971769,  0.03138824],\n",
       "       [ 0.79524064,  0.54144043,  0.27072022,  0.03384003],\n",
       "       [ 0.80846584,  0.52213419,  0.26948861,  0.03368608],\n",
       "       [ 0.82225028,  0.51771314,  0.22840286,  0.06090743],\n",
       "       [ 0.76578311,  0.60379053,  0.22089897,  0.0147266 ],\n",
       "       [ 0.77867447,  0.59462414,  0.19820805,  0.02831544],\n",
       "       [ 0.81803119,  0.51752994,  0.25041771,  0.01669451],\n",
       "       [ 0.82512295,  0.52807869,  0.19802951,  0.03300492],\n",
       "       [ 0.82699754,  0.52627116,  0.19547215,  0.03007264],\n",
       "       [ 0.81803119,  0.51752994,  0.25041771,  0.01669451],\n",
       "       [ 0.80212413,  0.54690282,  0.23699122,  0.03646019],\n",
       "       [ 0.80779568,  0.53853046,  0.23758697,  0.03167826],\n",
       "       [ 0.80033301,  0.56023311,  0.20808658,  0.04801998],\n",
       "       [ 0.86093857,  0.44003527,  0.24871559,  0.0573959 ],\n",
       "       [ 0.78609038,  0.57170209,  0.23225397,  0.03573138],\n",
       "       [ 0.78889479,  0.55222635,  0.25244633,  0.09466737],\n",
       "       [ 0.76693897,  0.57144472,  0.28572236,  0.06015208],\n",
       "       [ 0.82210585,  0.51381615,  0.23978087,  0.05138162],\n",
       "       [ 0.77729093,  0.57915795,  0.24385598,  0.030482  ],\n",
       "       [ 0.79594782,  0.55370283,  0.24224499,  0.03460643],\n",
       "       [ 0.79837025,  0.55735281,  0.22595384,  0.03012718],\n",
       "       [ 0.81228363,  0.5361072 ,  0.22743942,  0.03249135],\n",
       "       [ 0.76701103,  0.35063361,  0.51499312,  0.15340221],\n",
       "       [ 0.74549757,  0.37274878,  0.52417798,  0.17472599],\n",
       "       [ 0.75519285,  0.33928954,  0.53629637,  0.16417236],\n",
       "       [ 0.75384916,  0.31524601,  0.54825394,  0.17818253],\n",
       "       [ 0.7581754 ,  0.32659863,  0.5365549 ,  0.17496355],\n",
       "       [ 0.72232962,  0.35482858,  0.57026022,  0.16474184],\n",
       "       [ 0.72634846,  0.38046824,  0.54187901,  0.18446945],\n",
       "       [ 0.75916547,  0.37183615,  0.51127471,  0.15493173],\n",
       "       [ 0.76301853,  0.33526572,  0.53180079,  0.15029153],\n",
       "       [ 0.72460233,  0.37623583,  0.54345175,  0.19508524],\n",
       "       [ 0.76923077,  0.30769231,  0.53846154,  0.15384615],\n",
       "       [ 0.73923462,  0.37588201,  0.52623481,  0.187941  ],\n",
       "       [ 0.78892752,  0.28927343,  0.52595168,  0.13148792],\n",
       "       [ 0.73081412,  0.34743622,  0.56308629,  0.16772783],\n",
       "       [ 0.75911707,  0.3931142 ,  0.48800383,  0.17622361],\n",
       "       [ 0.76945444,  0.35601624,  0.50531337,  0.16078153],\n",
       "       [ 0.70631892,  0.37838513,  0.5675777 ,  0.18919257],\n",
       "       [ 0.75676497,  0.35228714,  0.53495455,  0.13047672],\n",
       "       [ 0.76444238,  0.27125375,  0.55483721,  0.18494574],\n",
       "       [ 0.76185188,  0.34011245,  0.53057542,  0.14964948],\n",
       "       [ 0.6985796 ,  0.37889063,  0.56833595,  0.21312598],\n",
       "       [ 0.77011854,  0.35349703,  0.50499576,  0.16412362],\n",
       "       [ 0.74143307,  0.29421947,  0.57667016,  0.17653168],\n",
       "       [ 0.73659895,  0.33811099,  0.56754345,  0.14490471],\n",
       "       [ 0.76741698,  0.34773582,  0.51560829,  0.15588157],\n",
       "       [ 0.76785726,  0.34902603,  0.51190484,  0.16287881],\n",
       "       [ 0.76467269,  0.31486523,  0.53976896,  0.15743261],\n",
       "       [ 0.74088576,  0.33173989,  0.55289982,  0.18798594],\n",
       "       [ 0.73350949,  0.35452959,  0.55013212,  0.18337737],\n",
       "       [ 0.78667474,  0.35883409,  0.48304589,  0.13801311],\n",
       "       [ 0.76521855,  0.33391355,  0.52869645,  0.15304371],\n",
       "       [ 0.77242925,  0.33706004,  0.51963422,  0.14044168],\n",
       "       [ 0.76434981,  0.35581802,  0.51395936,  0.15814134],\n",
       "       [ 0.70779525,  0.31850786,  0.60162596,  0.1887454 ],\n",
       "       [ 0.69333409,  0.38518561,  0.57777841,  0.1925928 ],\n",
       "       [ 0.71524936,  0.40530797,  0.53643702,  0.19073316],\n",
       "       [ 0.75457341,  0.34913098,  0.52932761,  0.16893434],\n",
       "       [ 0.77530021,  0.28304611,  0.54147951,  0.15998258],\n",
       "       [ 0.72992443,  0.39103094,  0.53440896,  0.16944674],\n",
       "       [ 0.74714194,  0.33960997,  0.54337595,  0.17659719],\n",
       "       [ 0.72337118,  0.34195729,  0.57869695,  0.15782644],\n",
       "       [ 0.73260391,  0.36029701,  0.55245541,  0.1681386 ],\n",
       "       [ 0.76262994,  0.34186859,  0.52595168,  0.1577855 ],\n",
       "       [ 0.76986879,  0.35413965,  0.5081134 ,  0.15397376],\n",
       "       [ 0.73544284,  0.35458851,  0.55158213,  0.1707278 ],\n",
       "       [ 0.73239618,  0.38547167,  0.53966034,  0.15418867],\n",
       "       [ 0.73446047,  0.37367287,  0.5411814 ,  0.16750853],\n",
       "       [ 0.75728103,  0.3542121 ,  0.52521104,  0.15878473],\n",
       "       [ 0.78258054,  0.38361791,  0.4603415 ,  0.16879188],\n",
       "       [ 0.7431482 ,  0.36505526,  0.5345452 ,  0.16948994],\n",
       "       [ 0.65387747,  0.34250725,  0.62274045,  0.25947519],\n",
       "       [ 0.69052512,  0.32145135,  0.60718588,  0.22620651],\n",
       "       [ 0.71491405,  0.30207636,  0.59408351,  0.21145345],\n",
       "       [ 0.69276796,  0.31889319,  0.61579374,  0.1979337 ],\n",
       "       [ 0.68619022,  0.31670318,  0.61229281,  0.232249  ],\n",
       "       [ 0.70953708,  0.28008043,  0.61617694,  0.1960563 ],\n",
       "       [ 0.67054118,  0.34211284,  0.61580312,  0.23263673],\n",
       "       [ 0.71366557,  0.28351098,  0.61590317,  0.17597233],\n",
       "       [ 0.71414125,  0.26647062,  0.61821183,  0.19185884],\n",
       "       [ 0.69198788,  0.34599394,  0.58626751,  0.24027357],\n",
       "       [ 0.71562645,  0.3523084 ,  0.56149152,  0.22019275],\n",
       "       [ 0.71576546,  0.30196356,  0.59274328,  0.21249287],\n",
       "       [ 0.71718148,  0.31640359,  0.58007326,  0.22148252],\n",
       "       [ 0.6925518 ,  0.30375079,  0.60750157,  0.24300063],\n",
       "       [ 0.67767924,  0.32715549,  0.59589036,  0.28041899],\n",
       "       [ 0.69589887,  0.34794944,  0.57629125,  0.25008866],\n",
       "       [ 0.70610474,  0.3258945 ,  0.59747324,  0.1955367 ],\n",
       "       [ 0.69299099,  0.34199555,  0.60299216,  0.19799743],\n",
       "       [ 0.70600618,  0.2383917 ,  0.63265489,  0.21088496],\n",
       "       [ 0.72712585,  0.26661281,  0.60593821,  0.18178146],\n",
       "       [ 0.70558934,  0.32722984,  0.58287815,  0.23519645],\n",
       "       [ 0.68307923,  0.34153961,  0.59769433,  0.24395687],\n",
       "       [ 0.71486543,  0.25995106,  0.62202576,  0.18567933],\n",
       "       [ 0.73122464,  0.31338199,  0.56873028,  0.20892133],\n",
       "       [ 0.69595601,  0.3427843 ,  0.59208198,  0.21813547],\n",
       "       [ 0.71529453,  0.31790868,  0.59607878,  0.17882363],\n",
       "       [ 0.72785195,  0.32870733,  0.56349829,  0.21131186],\n",
       "       [ 0.71171214,  0.35002236,  0.57170319,  0.21001342],\n",
       "       [ 0.69594002,  0.30447376,  0.60894751,  0.22835532],\n",
       "       [ 0.73089855,  0.30454106,  0.58877939,  0.1624219 ],\n",
       "       [ 0.72766159,  0.27533141,  0.59982915,  0.18683203],\n",
       "       [ 0.71578999,  0.34430405,  0.5798805 ,  0.18121266],\n",
       "       [ 0.69417747,  0.30370264,  0.60740528,  0.2386235 ],\n",
       "       [ 0.72366005,  0.32162669,  0.58582004,  0.17230001],\n",
       "       [ 0.69385414,  0.29574111,  0.63698085,  0.15924521],\n",
       "       [ 0.73154399,  0.28501714,  0.57953485,  0.21851314],\n",
       "       [ 0.67017484,  0.36168166,  0.59571097,  0.2553047 ],\n",
       "       [ 0.69804799,  0.338117  ,  0.59988499,  0.196326  ],\n",
       "       [ 0.71066905,  0.35533453,  0.56853524,  0.21320072],\n",
       "       [ 0.72415258,  0.32534391,  0.56672811,  0.22039426],\n",
       "       [ 0.69997037,  0.32386689,  0.58504986,  0.25073566],\n",
       "       [ 0.73337886,  0.32948905,  0.54206264,  0.24445962],\n",
       "       [ 0.69052512,  0.32145135,  0.60718588,  0.22620651],\n",
       "       [ 0.69193502,  0.32561648,  0.60035539,  0.23403685],\n",
       "       [ 0.68914871,  0.33943145,  0.58629069,  0.25714504],\n",
       "       [ 0.72155725,  0.32308533,  0.56001458,  0.24769876],\n",
       "       [ 0.72965359,  0.28954508,  0.57909015,  0.22005426],\n",
       "       [ 0.71653899,  0.3307103 ,  0.57323119,  0.22047353],\n",
       "       [ 0.67467072,  0.36998072,  0.58761643,  0.25028107],\n",
       "       [ 0.69025916,  0.35097923,  0.5966647 ,  0.21058754]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The normalizer instance can then be used on sample vectors as any transformer:\n",
    "\"\"\"\n",
    "normalizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70710678,  0.70710678,  0.        ]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer.transform([[-1., 1., 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Binarizer(copy=True, threshold=0.0)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Sparse input:\n",
    "    \n",
    "    normalize and Normalizer accept both dense array-like and sparse matrices from scipy.sparse as input.\n",
    "    \n",
    "    For sparse input the data is converted to the Compressed Sparse Rows representation \n",
    "    (see scipy.sparse.csr_matrix) before being fed to efficient Cython routines. To avoid unnecessary \n",
    "    memory copies, it is recommended to choose the CSR representation upstream.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "4.3.4 BINARIZATION\n",
    "\n",
    "4.3.4.1 FEATURES BINARIZATION\n",
    "    \n",
    "    Feature binarization is the process of thresholding numerical features to get boolean values. \n",
    "    This can be useful for downstream probabilistic estimators that make assumption that the input data \n",
    "    is distributed according to a multi-variate Bernoulli distribution. For instance, this is the case for \n",
    "    the sklearn.neural_network.BernoulliRBM.\n",
    "    \n",
    "    It is also common among the text processing community to use binary feature values (probably to \n",
    "    simplify the probabilistic reasoning) even if normalized counts (a.k.a. term frequencies) or \n",
    "    TF-IDF valued features often perform slightly better in practice.\n",
    "\n",
    "    As for the Normalizer, the utility class Binarizer is meant to be used in the early stages of \n",
    "    sklearn.pipeline.Pipeline. The fit method does nothing as each sample is treated independently of others:\n",
    "\n",
    "\"\"\"\n",
    "binarizer = preprocessing.Binarizer().fit(X)\n",
    "binarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  0.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  0.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.],\n",
       "       [ 1.,  1.,  1.,  1.]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    It is possible to adjust the threshold of the binarizer:\n",
    "\"\"\"\n",
    "binarizer = preprocessing.Binarizer(threshold=1.1)\n",
    "binarizer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    As for the StandardScaler and Normalizer classes, the preprocessing module provides a companion \n",
    "    function binarize to be used when the transformer API is not necessary.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "    SPARSE INPUT\n",
    "    \n",
    "    binarize and Binarizer accept both dense array-like and sparse matrices from scipy.sparse as input.\n",
    "    \n",
    "    For sparse input the data is converted to the Compressed Sparse Rows representation \n",
    "    (see scipy.sparse.csr_matrix). To avoid unnecessary memory copies, it is recommended to choose the \n",
    "    CSR representation upstream.\n",
    "\"\"\"\n",
    "print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3.5 ENCODING CATEGORICAL FEATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values='auto', sparse=True)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Often features are not given as continuous values but categorical. For example a person could \n",
    "    have features [\"male\", \"female\"], [\"from Europe\", \"from US\", \"from Asia\"], [\"uses Firefox\", \"uses Chrome\", \n",
    "    \"uses Safari\", \"uses Internet Explorer\"]. Such features can be efficiently coded as integers, \n",
    "    for instance [\"male\", \"from US\", \"uses Internet Explorer\"] could be expressed as [0, 1, 3] while \n",
    "    [\"female\", \"from Asia\", \"uses Chrome\"] would be [1, 2, 1].\n",
    "\n",
    "    Such integer representation can not be used directly with scikit-learn estimators, as these expect \n",
    "    continuous input, and would interpret the categories as being ordered, which is often not desired \n",
    "    (i.e. the set of browsers was ordered arbitrarily).\n",
    "\n",
    "    One possibility to convert categorical features to features that can be used with scikit-learn \n",
    "    estimators is to use a one-of-K or one-hot encoding, which is implemented in OneHotEncoder. \n",
    "    This estimator transforms each categorical feature with m possible values into m binary features, \n",
    "    with only one active.\n",
    "\n",
    "\"\"\"\n",
    "enc = preprocessing.OneHotEncoder()\n",
    "enc.fit([[0, 0, 3], [1, 1, 0], [0, 2, 1], [1, 0, 2]])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.,  0.,  0.,  1.,  0.,  0.,  0.,  0.,  1.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.transform([[0, 1, 3]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OneHotEncoder(categorical_features='all', dtype=<class 'numpy.float64'>,\n",
       "       handle_unknown='error', n_values=[2, 3, 4], sparse=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    By default, how many values each feature can take is inferred automatically from the dataset. \n",
    "    It is possible to specify this explicitly using the parameter n_values. There are two genders, \n",
    "    three possible continents and four web browsers in our dataset. Then we fit the estimator, and \n",
    "    transform a data point. In the result, the first two numbers encode the gender, the next set of three\n",
    "    numbers the continent and the last four the web browser.\n",
    "\n",
    "    Note that, if there is a possibility that the training data might have missing categorical features, \n",
    "    one has to explicitly set n_values. For example,\n",
    "\"\"\"\n",
    "enc = preprocessing.OneHotEncoder(n_values=[2, 3, 4])\n",
    "# Nore that here are missing categorical values for the 2nd and 3rd features\n",
    "enc.fit([[1, 2, 3],[0, 2, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  1.,  1.,  0.,  0.,  1.,  0.,  0.,  0.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc.transform([[1, 0, 0]]).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imputer(axis=0, copy=True, missing_values='NaN', strategy='mean', verbose=0)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    See Loading features from dicts for categorical features that are represented as a dict, not as integers.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "4.3.6 IMPUTATION OF MISSING VALUES\n",
    "\n",
    "    For various reasons, many real world datasets contain missing values, often encoded as blanks, \n",
    "    NaNs or other placeholders. Such datasets however are incompatible with scikit-learn estimators \n",
    "    which assume that all values in an array are numerical, and that all have and hold meaning. A basic \n",
    "    strategy to use incomplete datasets is to discard entire rows and/or columns containing missing values. \n",
    "    However, this comes at the price of losing data which may be valuable (even though incomplete). A better \n",
    "    strategy is to impute the missing values, i.e., to infer them from the known part of the data.\n",
    "    \n",
    "    The Imputer class provides basic strategies for imputing missing values, either using the mean, \n",
    "    the median or the most frequent value of the row or column in which the missing values are located. \n",
    "    This class also allows for different missing values encodings.\n",
    "    \n",
    "    The following snippet demonstrates how to replace missing values, encoded as np.nan, using the mean \n",
    "    value of the columns (axis 0) that contain the missing values:\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import Imputer\n",
    "imp = Imputer(missing_values = 'NaN', strategy = 'mean', axis = 0)\n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.          2.        ]\n",
      " [ 6.          3.66666667]\n",
      " [ 7.          6.        ]]\n"
     ]
    }
   ],
   "source": [
    "X = [[np.nan, 2], [6, np.nan], [7, 6]]\n",
    "print(imp.transform(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Imputer(axis=0, copy=True, missing_values=0, strategy='mean', verbose=0)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The Imputer class also supports sparse matrices:\n",
    "\"\"\"\n",
    "import scipy.sparse as sp\n",
    "X = sp.csc_matrix([[1, 2], [0, 3], [7, 6]])\n",
    "imp = Imputer(missing_values=0, strategy='mean', axis=0)\n",
    "imp.fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.          2.        ]\n",
      " [ 6.          3.66666667]\n",
      " [ 7.          6.        ]]\n"
     ]
    }
   ],
   "source": [
    "X_test = sp.csc_matrix([[0, 2], [6, 0], [7, 6]])\n",
    "print(imp.transform(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1],\n",
       "       [2, 3],\n",
       "       [4, 5]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    Note that, here, missing values are encoded by 0 and are thus implicitly stored in the matrix. \n",
    "    This format is thus suitable when there are many more missing values than observed values.\n",
    "    \n",
    "    Imputer can be used in a Pipeline as a way to build a composite estimator that supports imputation. \n",
    "    See Imputing missing values before building an estimator.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "\n",
    "5.3.7 GENERATIONG POLYNOMIAL FEATURES\n",
    "    Often it’s useful to add complexity to the model by considering nonlinear features of the input data.\n",
    "    A simple and common method to use is polynomial features, which can get features’ high-order and \n",
    "    interaction terms. It is implemented in PolynomialFeatures:\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "X = np.arange(6).reshape(3,2)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.,   0.,   1.,   0.,   0.,   1.],\n",
       "       [  1.,   2.,   3.,   4.,   6.,   9.],\n",
       "       [  1.,   4.,   5.,  16.,  20.,  25.]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(2)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 2],\n",
       "       [3, 4, 5],\n",
       "       [6, 7, 8]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The features of X have been transformed from (X_1, X_2) to (1, X_1, X_2, X_1^2, X_1X_2, X_2^2).\n",
    "\n",
    "    In some cases, only interaction terms among features are required, and it can be gotten with the \n",
    "    setting interaction_only=True:\n",
    "\"\"\"\n",
    "X = np.arange(9).reshape(3, 3)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   1.,    0.,    1.,    2.,    0.,    0.,    2.,    0.],\n",
       "       [   1.,    3.,    4.,    5.,   12.,   15.,   20.,   60.],\n",
       "       [   1.,    6.,    7.,    8.,   42.,   48.,   56.,  336.]])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poly = PolynomialFeatures(degree=3, interaction_only=True)\n",
    "poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.        ,  0.69314718],\n",
       "       [ 1.09861229,  1.38629436]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    The features of X have been transformed from (X_1, X_2, X_3) to \n",
    "    (1, X_1, X_2, X_3, X_1X_2, X_1X_3, X_2X_3, X_1X_2X_3).\n",
    "\n",
    "    Note that polynomial features are used implicitly in kernel methods (e.g., sklearn.svm.SVC, \n",
    "    sklearn.decomposition.KernelPCA) when using polynomial Kernel functions.\n",
    "\n",
    "    See Polynomial interpolation for Ridge regression using created polynomial features.\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "4.3.8 CUSTOM TRANSFORMERS\n",
    "\n",
    "    Often, you will want to convert an existing Python function into a transformer to assist \n",
    "    in data cleaning or processing. You can implement a transformer from an arbitrary function\n",
    "    with FunctionTransformer. For example, to build a transformer that applies a log transformation\n",
    "    in a pipeline, do:\n",
    "\"\"\"\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "transformer = FunctionTransformer(np.log1p)\n",
    "X = np.array([[0, 1], [2, 3]])\n",
    "transformer.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n    For a full code example that demonstrates using a FunctionTransformer to do custom feature selection,\\n    see Using FunctionTransformer to select columns\\n'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "    For a full code example that demonstrates using a FunctionTransformer to do custom feature selection,\n",
    "    see Using FunctionTransformer to select columns\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
